{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Instantiating an environment{vertical-output: true}\n",
    "import car\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "# from NavTask import Navigation\n",
    "from ButtonTask import PressWithSpecificForce\n",
    "from dm_control import viewer\n",
    "from dm_control import composer\n",
    "import numpy as np\n",
    "\n",
    "creature = car.Car()\n",
    "task = PressWithSpecificForce(creature)\n",
    "# task = CarTask()\n",
    "env = composer.Environment(task, random_state=np.random.RandomState(1))\n",
    "\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.physics.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ButtonTask import PressWithSpecificForce\n",
    "# from NavTask import Navigation\n",
    "from SurvivalTask import Survive\n",
    "# from MazeTask import Maze\n",
    "from dm_control import viewer\n",
    "from dm_control import composer\n",
    "import numpy as np\n",
    "from car import Car\n",
    "from Point_Cloud_Env import CarEnv\n",
    "\n",
    "creature = Car()\n",
    "\n",
    "task = Survive(creature,150,9)\n",
    "# task = Maze(creature)\n",
    "def make_env():\n",
    "    return CarEnv()\n",
    "\n",
    "env = make_env()\n",
    "# original_env = env.original_env\n",
    "# task = PressWithSpecificForce(creature)\n",
    "original_env = composer.Environment(task, raise_exception_on_physics_error=False, strip_singleton_obs_buffer_dim=True)\n",
    "\n",
    "print(original_env.physics.data.ncon)\n",
    "def random_policy(time_step):\n",
    "# del time_step # Unused.\n",
    "# return np.random.uniform(low=np.array([-0.38, -1.]), high=np.array([0.38, 3.]), size=(2,))\n",
    "    return np.array([0 , 1])\n",
    "viewer.launch(original_env, policy=random_policy)\n",
    "\n",
    "# task.initialize_episode(original_env.physics,None)\n",
    "# viewer.launch(original_env, policy=random_policy)\n",
    "\n",
    "# collisions = task.detect_collisions(original_env.physics)\n",
    "# print(collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Point_Cloud_Env import CarEnv\n",
    "\n",
    "# env = CarEnv()  # or Point_Cloud_Env()\n",
    "# obs, _ = env.reset()\n",
    "# for _ in range(100):\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     if done or truncated:\n",
    "#         obs, _ = env.reset()\n",
    "# print(\"Single environment runs smoothly.\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Testing initialization {i}\")\n",
    "    env = CarEnv()\n",
    "    obs, _ = env.reset()\n",
    "    env.close()\n",
    "print(\"Repeated environment creation succeeded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "# from ButtonEnv import CarEnv\n",
    "# from NavEnv import CarEnv\n",
    "# from ImageEnv import CarEnv\n",
    "from Point_Cloud_Env import CarEnv\n",
    "from GNN_Arch import point_cloud_to_graph, count_neighbors\n",
    "import torch\n",
    "from scipy.spatial import distance_matrix\n",
    "from GNN_Arch import visualize_graph\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "def make_env():\n",
    "    return CarEnv()\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "\n",
    "# action = np.array([0.2,0])\n",
    "# env.step(action)\n",
    "obs = env.getCurrObs()\n",
    "point_cloud = obs['pc']\n",
    "\n",
    "# print(point_cloud)\n",
    "\n",
    "# Compute pairwise distances\n",
    "# dist_matrix = distance_matrix(point_cloud, point_cloud)\n",
    "\n",
    "# # Exclude self-distances (set diagonal to a high value)\n",
    "# np.fill_diagonal(dist_matrix, np.inf)\n",
    "\n",
    "# # Find the closest non-self distances for each point\n",
    "# min_distances = np.min(dist_matrix, axis=1)\n",
    "\n",
    "# # Determine an appropriate radius as a percentile of these distances\n",
    "# recommended_radius = np.percentile(min_distances, 95)  # Using the 95th percentile\n",
    "\n",
    "# # Determine max_num_neighbors based on typical connectivity\n",
    "# max_num_neighbors = int(np.percentile(np.sum(dist_matrix < recommended_radius, axis=1), 95))\n",
    "\n",
    "# print(recommended_radius, max_num_neighbors)\n",
    "\n",
    "# np.save(\"point_cloud.npy\", point_cloud)  # Save as NumPy array\n",
    "\n",
    "# mean = point_cloud.mean(axis=0,keepdims=True)\n",
    "# std = point_cloud.std(axis=0,keepdims=True) + 1e-6\n",
    "# point_cloud = (point_cloud - mean) / std\n",
    "# print('maximum x:',np.max(point_cloud[:,0]), 'maximum y:', np.max(point_cloud[:,1]))\n",
    "# print(np.min(obs['pc'][:,2]),np.max(obs['pc'][:,2]))\n",
    "\n",
    "edge_index, vertex_features, batch = point_cloud_to_graph(torch.tensor(obs['pc']).unsqueeze(0), method='knn', k = 8, radius=2)\n",
    "print('max x:',vertex_features[:,0].max(), 'max y:',vertex_features[:,1].max(), 'max z:',vertex_features[:,2].max())\n",
    "print('min x:',vertex_features[:,0].min(), 'min y:',vertex_features[:,1].min(), 'min z:',vertex_features[:,2].min())\n",
    "visualize_graph(point_cloud,edge_index)\n",
    "# count = count_neighbors(point_graph.edge_index,64*128)\n",
    "# print(count,torch.sum(count==max(count)),min(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "default_fovy_degrees = 45.0\n",
    "default_fovy_radians = np.deg2rad(default_fovy_degrees)\n",
    "height = 64\n",
    "width = 128\n",
    "aspect_ratio = 1  # Explicitly calculate aspect ratio\n",
    "\n",
    "fy_calculated = height / (2 * np.tan(default_fovy_radians / 2))\n",
    "fx_calculated = fy_calculated * aspect_ratio\n",
    "cx_calculated = width / 2\n",
    "cy_calculated = height / 2\n",
    "\n",
    "print(\"Calculated Camera Intrinsics (based on default fovy=45 and aspect=width/height):\")\n",
    "print(f\"fx: {fx_calculated:.2f}\")\n",
    "print(f\"fy: {fy_calculated:.2f}\")\n",
    "print(f\"cx: {cx_calculated:.2f}\")\n",
    "print(f\"cy: {cy_calculated:.2f}\")\n",
    "\n",
    "print(\"\\nOriginal Hardcoded Intrinsics (from car.py):\")\n",
    "print(f\"fx_hardcoded: 382.57\")\n",
    "print(f\"fy_hardcoded: 382.57\")\n",
    "print(f\"cx_hardcoded: {width / 2}\")\n",
    "print(f\"cy_hardcoded: {height / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmkr/LocoTransformer_v2/NewEnv/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import SAC, PPO\n",
    "# from sbx import SAC\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NavEnv import CarEnv\n",
    "# from ButtonEnv import CarEnv\n",
    "# from SurvivalEnv import CarEnv\n",
    "from ImageEnv import CarEnv\n",
    "# from Point_Cloud_Env import CarEnv\n",
    "from feature_extractors import PointGNNFeatureExtractorWrapper, CNNFeatureExtractor\n",
    "import os\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# os.environ[\"MJ_THREADS\"] = \"4\"\n",
    "\n",
    "\n",
    "def make_env(rank):\n",
    "    def _init():\n",
    "        # print(f\"Initializing environment {rank} in process {os.getpid()}\")\n",
    "        return CarEnv()\n",
    "    return _init\n",
    "\n",
    "tb = os.path.join(\"/home/mmkr/LocoTransformer_v2/Pushr_car_simulation/results/tensor\",\"SAC_2DCNN_300k\")\n",
    "models = os.path.join(\"/home/mmkr/LocoTransformer_v2/Pushr_car_simulation/results/models\",\"SAC_2DCNN_300k\")\n",
    "stats_path = os.path.join(\"/home/mmkr/LocoTransformer_v2/Pushr_car_simulation/results/logs\",\"SAC_2DCNN_300k\")\n",
    "\n",
    "num_envs = 8\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
    "# env = DummyVecEnv([make_env(i) for i in range(num_envs)])\n",
    "env = VecNormalize(env, norm_obs=False, norm_reward=True)\n",
    "\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     features_extractor_class=PointGNNFeatureExtractorWrapper,\n",
    "#     features_extractor_kwargs=dict(input_dim=3,hidden_dim=32,output_dim=256,num_layers=3,use_edgeconv=False),\n",
    "#     net_arch=[256,256]\n",
    "# )\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CNNFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    "    net_arch=[256,256]\n",
    ")\n",
    "\n",
    "\n",
    "# model = PPO(\n",
    "#     policy='MultiInputPolicy',\n",
    "#     n_steps = 2048,\n",
    "#     learning_rate=3e-4,\n",
    "#     n_epochs=10,\n",
    "#     env=env,\n",
    "#     batch_size=16,\n",
    "#     clip_range=0.2,\n",
    "#     ent_coef=0.01,\n",
    "#     vf_coef=0.5,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=tb,\n",
    "#     seed=1,\n",
    "#     policy_kwargs=policy_kwargs\n",
    "# )\n",
    "\n",
    "\n",
    "model = SAC(\n",
    "    policy='MultiInputPolicy',\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=128,\n",
    "    buffer_size=150000,\n",
    "    learning_starts=1000,\n",
    "    tensorboard_log=tb,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# saved_model_path='/home/mmkr/LocoTransformer_v2/Pushr_car_simulation/results/models/PPO_GS_100k_32_128'\n",
    "# model=PPO.load(saved_model_path, env=env)\n",
    "# model.policy.half()\n",
    "\n",
    "# with torch.amp.autocast(device_type=\"cuda\"):\n",
    "model.learn(total_timesteps=300000)\n",
    "env.save(stats_path+'test'+\".pkl\")\n",
    "print(\"Saving end model\")\n",
    "model.save(models)\n",
    "\n",
    "# model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmkr/LocoTransformer_v2/NewEnv/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Dist 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import SAC, PPO\n",
    "# from sbx import SAC\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import torch.optim as optim\n",
    "\n",
    "import car\n",
    "import numpy as np\n",
    "from Point_Cloud_Env import CarEnv\n",
    "# from ImageEnv import CarEnv\n",
    "from SurvivalTask import Survive\n",
    "from dm_control import viewer,composer\n",
    "\n",
    "env=CarEnv()\n",
    "\n",
    "task = env.task\n",
    "# task = Maze(creature)\n",
    "# task = PressWithSpecificForce(creature)\n",
    "original_env = env.original_env\n",
    "\n",
    "saved_model_path='results/models/SAC_GC_700k_4N_32_256.zip'\n",
    "model=SAC.load(saved_model_path, env=env, custom_objects={'buffer_size': 1})\n",
    "# model.replay_buffer = None\n",
    "# model.set_env(env=env)\n",
    "state,_ = env.reset()\n",
    "# # model.policy\n",
    "\n",
    "def policy(time_step,model):\n",
    "    \n",
    "    pc = time_step.observation['car/compute_point_cloud']\n",
    "    obs = {\"pc\": pc}\n",
    "    # vec = time_step.observation['car/body_vel_2d']\n",
    "    # img = time_step.observation['car/realsense_camera']\n",
    "    # obs = {\"vec\":vec, \"img\":img}\n",
    "\n",
    "    action, _=model.predict(obs,deterministic=True)\n",
    "    # obs, reward, done, _,_ = env.step(action)\n",
    "    # print(action)\n",
    "    \n",
    "    return action\n",
    "\n",
    "viewer.launch(original_env, policy=lambda timestep: policy(timestep, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from stable_baselines3 import SAC, PPO\n",
    "# from sb3_contrib import RecurrentPPO\n",
    "ext_list = [\"pi_features_extractor\", \"mlp_extractor.policy_net\", \"action_net\"]\n",
    "\"\"\"\n",
    "pi_features_extractor:\n",
    "    Input: Depth image format -> (1,128,128,1)\n",
    "    output: 128 features ---- check for the shape of the output\n",
    "mlp_extractor.policy_net:\n",
    "    input: pi_features_extractor_output --> 128\n",
    "    output: 128 ----> check for the shape of the output\n",
    "action_net:\n",
    "    input: mlp_extractor.policy_net --> 128\n",
    "    output: 2 ----> check for the shape of the output\n",
    "\"\"\"\n",
    "class DictInputWrapper(torch.nn.Module):\n",
    "    def __init__(self, policy):\n",
    "        super(DictInputWrapper, self).__init__()\n",
    "        self.policy = policy\n",
    "    def forward(self, depth, vec):\n",
    "        # Recreate the observation dictionary\n",
    "        obs = {'vec': vec, 'img': depth}\n",
    "        # Forward pass through the actual policy using dict observation\n",
    "        return self.policy(obs)\n",
    "    \n",
    "    \n",
    "def export_to_onnx(model_part, input_tensors, output_path, input_names=None, output_names=None):\n",
    "    # Use the provided input names or default to \"input1\", \"input2\", etc.\n",
    "    if input_names is None:\n",
    "        input_names = [f\"input_{i}\" for i in range(len(input_tensors))]\n",
    "    if output_names is None:\n",
    "        output_names = [\"output\"]\n",
    "\n",
    "    # Prepare dynamic axes for each input\n",
    "    dynamic_axes = {name: {0: 'batch_size'} for name in input_names}\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model_part,\n",
    "        input_tensors,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes\n",
    "    )\n",
    "    print(f\"Exported {output_path} to ONNX successfully!\")\n",
    "\n",
    "\n",
    "def save_model_parts_as_onnx(model, output_dir=\"onnx_models\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.policy.eval()\n",
    "\n",
    "    # Determine the device (CPU or GPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Extract CNN Network (if it only processes 'img')\n",
    "    print(\"Extracting and converting CNN Network...\")\n",
    "    dummy_img = torch.randn(1, *model.observation_space['img'].shape).to(device)\n",
    "    cnn_wrapper = DictInputWrapper(model.policy.features_extractor)\n",
    "    export_to_onnx(\n",
    "        cnn_wrapper,\n",
    "        (torch.zeros_like(dummy_img), dummy_img),  # vec is zeros\n",
    "        os.path.join(output_dir, \"CNN.onnx\"),\n",
    "        input_names=[\"vec\", \"img\"],\n",
    "        output_names=[\"output\"]\n",
    "    )\n",
    "\n",
    "    # Extract MLP Network (if it processes combined features)\n",
    "    print(\"Extracting and converting MLP Network...\")\n",
    "    # Determine the input size for the MLP; this depends on your model architecture\n",
    "    # For example, if the MLP processes features of size N, create a dummy input of that size\n",
    "    dummy_features = torch.randn(1, model.policy.mlp_extractor.policy_net[0].in_features).to(device)\n",
    "    export_to_onnx(\n",
    "        model.policy.mlp_extractor.policy_net,\n",
    "        dummy_features,\n",
    "        os.path.join(output_dir, \"MLP.onnx\"),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"]\n",
    "    )\n",
    "\n",
    "    # Extract Action Network\n",
    "    print(\"Extracting and converting Action Network...\")\n",
    "    export_to_onnx(\n",
    "        model.policy.action_net,\n",
    "        dummy_features,\n",
    "        os.path.join(output_dir, \"ActionNet.onnx\"),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def load_model_and_save_parts(model_type, model_path, output_dir):\n",
    "    # model_class = {\"PPO\": PPO, \"SAC\": SAC}[model_type]\n",
    "    # Load the model from .zip file to ensure weights are correct\n",
    "    model = PPO.load(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    # Save parts to ONNX\n",
    "    save_model_parts_as_onnx(model, output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description=\"Extract and convert parts of a saved model to ONNX format.\")\n",
    "    # parser.add_argument('--model_type', type=str, choices=[\"PPO\", \"SAC\", \"RecurrentPPO\"], help='Type of model to use', default=\"SAC\")\n",
    "    # parser.add_argument('--model_path', type=str, help='Path to the saved model (.zip) to extract parts from', required=True)\n",
    "    # parser.add_argument('--output_dir', type=str, help='Directory to save the ONNX files', default='./onnx_models')\n",
    "    # args = parser.parse_args()\n",
    "    model_type = PPO\n",
    "    model_path = \"/home/mmkr/Car_LocoTransformer/Mujoco_learning/Pushr_car_simulation/results/models/PPO_Img_Vel_300k\"\n",
    "    output_dir = \"/home/mmkr/Car_LocoTransformer/Mujoco_learning/Pushr_car_simulation/results/onnx\"\n",
    "    # Execute the extraction and conversion\n",
    "    # load_model_and_save_parts(args.model_type, args.model_path, args.output_dir)\n",
    "    load_model_and_save_parts(model_type, model_path, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
